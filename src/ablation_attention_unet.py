import torch
import torch.nn as nn
import torch.nn.functional as F

"""
Highly Configurable Attention U-Net
æ”¯æŒç‰¹æ€§ï¼š
1. use_smart_ratio: æ™ºèƒ½è°ƒæ•´é€šé“å‹ç¼©æ¯”ï¼Œé˜²æ­¢æµ…å±‚ç‰¹å¾ä¸¢å¤±ã€‚
2. use_residual: å¼€å¯æ®‹å·®è¿æ¥ (Res-UNetæ¨¡å¼)ã€‚
3. attention_mode: æ§åˆ¶æ³¨æ„åŠ›æ·»åŠ çš„ä½ç½® ('all', 'deep_only', 'none')ã€‚
"""

# ==========================================
# 1. æ”¹è¿›ç‰ˆ CBAM æ¨¡å—
# ==========================================

class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16, smart_ratio=False):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        # === [æ”¹è¿›æ–¹æ¡ˆä¸€] Smart Ratio ===
        # å¦‚æœå¼€å¯ smart_ratioï¼Œå¯¹äºå°‘äº64é€šé“çš„å±‚ï¼Œå¼ºåˆ¶é™ä½å‹ç¼©å€ç‡
        # ä¿è¯ä¸­é—´å±‚è‡³å°‘æœ‰ 8 ä¸ªé€šé“ï¼Œé¿å…ä¿¡æ¯ç“¶é¢ˆ
        if smart_ratio:
            if in_planes < 64: 
                real_ratio = 4 
            elif in_planes < 128:
                real_ratio = 8
            else:
                real_ratio = ratio
        else:
            real_ratio = ratio
            
        mid_planes = max(4, in_planes // real_ratio)
        
        self.fc1 = nn.Conv2d(in_planes, mid_planes, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Conv2d(mid_planes, in_planes, 1, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)

class CBAM(nn.Module):
    def __init__(self, in_planes, ratio=16, kernel_size=7, smart_ratio=False):
        super(CBAM, self).__init__()
        self.ca = ChannelAttention(in_planes, ratio, smart_ratio)
        self.sa = SpatialAttention(kernel_size)

    def forward(self, x):
        out = x * self.ca(x)
        result = out * self.sa(out)
        return result

# ==========================================
# 2. æ”¹è¿›ç‰ˆ å·ç§¯å— (æ”¯æŒæ®‹å·®)
# ==========================================

class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels, mid_channels=None, 
                 use_cbam=False, use_residual=False, smart_ratio=False):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
            
        self.use_cbam = use_cbam
        self.use_residual = use_residual

        # åŸºç¡€å·ç§¯éƒ¨åˆ†
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

        # CBAM æ¨¡å—
        if self.use_cbam:
            self.cbam = CBAM(out_channels, smart_ratio=smart_ratio)

        # === [æ”¹è¿›æ–¹æ¡ˆäºŒ] Residual Connection ===
        # å¦‚æœè¾“å…¥è¾“å‡ºé€šé“æ•°ä¸åŒï¼Œéœ€è¦ç”¨ 1x1 å·ç§¯è°ƒæ•´ Shortcut ç»´åº¦
        self.shortcut = nn.Sequential()
        if self.use_residual:
            if in_channels != out_channels:
                self.shortcut = nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
                    nn.BatchNorm2d(out_channels)
                )
            else:
                self.shortcut = nn.Identity()

    def forward(self, x):
        # 1. ä¸»å¹²è·¯å¾„
        out = self.conv(x)
        
        # 2. æ³¨æ„åŠ›æœºåˆ¶
        if self.use_cbam:
            out = self.cbam(out)
        
        # 3. æ®‹å·®è¿æ¥ (ResNetæ€æƒ³: H(x) = F(x) + x)
        if self.use_residual:
            out = out + self.shortcut(x)
            # æ³¨æ„ï¼šè¿™é‡Œçš„ ReLU ä¸€èˆ¬åœ¨ç›¸åŠ åä¸å†åšï¼Œå› ä¸º conv é‡Œé¢å·²ç» relu è¿‡äº†
            # å¦‚æœæƒ³ä¸¥æ ¼æ¨¡ä»¿ ResNetï¼Œå¯ä»¥åœ¨è¿™é‡Œå†åŠ ä¸€ä¸ª F.relu(out)
            
        return out

# ==========================================
# 3. è¾…åŠ©æ¨¡å— (Down, Up)
# ==========================================

class Down(nn.Module):
    def __init__(self, in_channels, out_channels, **kwargs):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_channels, out_channels, **kwargs) # ä¼ é€’é…ç½®å‚æ•°
        )

    def forward(self, x):
        return self.maxpool_conv(x)

class Up(nn.Module):
    def __init__(self, in_channels, out_channels, bilinear=True, **kwargs):
        super().__init__()
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2, **kwargs)
        else:
            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv(in_channels, out_channels, **kwargs)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]
        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
                        diffY // 2, diffY - diffY // 2])
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)

# ==========================================
# 4. å®Œæ•´çš„ Configurable U-Net
# ==========================================

class ConfigurableUNet(nn.Module):
    def __init__(self, n_channels, n_classes, bilinear=True, base_c=64, 
                 config=None):
        """
        config (dict): æ§åˆ¶å®éªŒå˜é‡çš„å­—å…¸
            - use_smart_ratio (bool): æ˜¯å¦å¼€å¯æ™ºèƒ½é€šé“å‹ç¼©
            - use_residual (bool): æ˜¯å¦å¼€å¯æ®‹å·®è¿æ¥
            - attention_mode (str): 'all' | 'deep_only' | 'none'
        """
        super(ConfigurableUNet, self).__init__()
        
        # é»˜è®¤é…ç½®
        default_config = {
            'use_smart_ratio': False,
            'use_residual': False,
            'attention_mode': 'all' # å¯é€‰: 'all', 'deep_only', 'none'
        }
        if config:
            default_config.update(config)
        self.cfg = default_config
        
        print(f"ğŸ”„ æ¨¡å‹åˆå§‹åŒ–é…ç½®: {self.cfg}")

        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear

        # === [æ”¹è¿›æ–¹æ¡ˆä¸‰] Attention Schedule ===
        # å®šä¹‰å“ªäº›å±‚å¼€å¯ CBAM
        def get_cbam_flag(layer_name):
            mode = self.cfg['attention_mode']
            if mode == 'none':
                return False
            if mode == 'all':
                return True
            if mode == 'deep_only':
                # å®šä¹‰æ·±å±‚ï¼šDown2, Down3, Down4, Up1, Up2
                # æµ…å±‚ï¼šInc, Down1, Up3, Up4 (ä¿ç•™é«˜åˆ†è¾¨ç‡ç‰¹å¾ä¸è¢«æŠ‘åˆ¶)
                if layer_name in ['down2', 'down3', 'down4', 'up1', 'up2']:
                    return True
                return False
            return False

        # æå–é€šç”¨å‚æ•°ï¼Œç®€åŒ–ä»£ç 
        common_args = {
            'use_residual': self.cfg['use_residual'],
            'smart_ratio': self.cfg['use_smart_ratio']
        }

        # ---- Encoder ----
        self.inc = DoubleConv(n_channels, base_c, 
                              use_cbam=get_cbam_flag('inc'), **common_args)
        
        self.down1 = Down(base_c, base_c * 2, 
                          use_cbam=get_cbam_flag('down1'), **common_args)
        
        self.down2 = Down(base_c * 2, base_c * 4, 
                          use_cbam=get_cbam_flag('down2'), **common_args)
        
        self.down3 = Down(base_c * 4, base_c * 8, 
                          use_cbam=get_cbam_flag('down3'), **common_args)
        
        factor = 2 if bilinear else 1
        self.down4 = Down(base_c * 8, base_c * 16 // factor, 
                          use_cbam=get_cbam_flag('down4'), **common_args)

        # ---- Decoder ----
        self.up1 = Up(base_c * 16, base_c * 8 // factor, bilinear, 
                      use_cbam=get_cbam_flag('up1'), **common_args)
        
        self.up2 = Up(base_c * 8, base_c * 4 // factor, bilinear, 
                      use_cbam=get_cbam_flag('up2'), **common_args)
        
        self.up3 = Up(base_c * 4, base_c * 2 // factor, bilinear, 
                      use_cbam=get_cbam_flag('up3'), **common_args)
        
        self.up4 = Up(base_c * 2, base_c, bilinear, 
                      use_cbam=get_cbam_flag('up4'), **common_args)
        
        self.outc = nn.Conv2d(base_c, n_classes, kernel_size=1)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        
        logits = self.outc(x)
        return logits

# ==========================================
# 5. æ§åˆ¶å˜é‡å®éªŒç¤ºä¾‹
# ==========================================

if __name__ == '__main__':
    # å‡è®¾è¾“å…¥
    x = torch.randn(2, 3, 256, 256)
    
    print("----- å®éªŒ 1: åŸå§‹ Attention U-Net (ä½ ä¹‹å‰çš„ç‰ˆæœ¬) -----")
    model_v1 = ConfigurableUNet(3, 1, config={
        'use_smart_ratio': False,
        'use_residual': False,
        'attention_mode': 'all'
    })
    # out = model_v1(x) # è·‘ä¸€ä¸‹ç¡®ä¿æ²¡æŠ¥é”™
    
    print("\n----- å®éªŒ 2: å¼€å¯ Smart Ratio (æ–¹æ¡ˆä¸€) -----")
    # æœŸæœ›ï¼šç¼“è§£æµ…å±‚ç‰¹å¾ä¸¢å¤±
    model_v2 = ConfigurableUNet(3, 1, config={
        'use_smart_ratio': True, 
        'use_residual': False,
        'attention_mode': 'all'
    })

    print("\n----- å®éªŒ 3: å¼€å¯ Residual Connection (æ–¹æ¡ˆäºŒ) -----")
    # æœŸæœ›ï¼šè®­ç»ƒæ›´ç¨³å®šï¼Œæ¢¯åº¦æ›´å¥½ä¼ å¯¼
    model_v3 = ConfigurableUNet(3, 1, config={
        'use_smart_ratio': False,
        'use_residual': True, # é‡ç‚¹
        'attention_mode': 'all'
    })

    print("\n----- å®éªŒ 4: åªåœ¨æ·±å±‚åŠ  Attention (æ–¹æ¡ˆä¸‰) -----")
    # æœŸæœ›ï¼šä¿ç•™æµ…å±‚çº¹ç†ï¼Œåªåœ¨è¯­ä¹‰å±‚åšç­›é€‰ï¼Œé€šå¸¸ IoU æœ€é«˜
    model_v4 = ConfigurableUNet(3, 1, config={
        'use_smart_ratio': False,
        'use_residual': False,
        'attention_mode': 'deep_only' # é‡ç‚¹
    })
    
    print("\n----- å®éªŒ 5: ç¼åˆæ€ª (å…¨å¼€ - æ¨è) -----")
    # ç»“åˆäº†æ‰€æœ‰ä¼˜ç‚¹
    model_final = ConfigurableUNet(3, 1, config={
        'use_smart_ratio': True,
        'use_residual': True,
        'attention_mode': 'deep_only'
    })
    
    out = model_final(x)
    print(f"\nâœ… æœ€ç»ˆè¾“å‡ºå°ºå¯¸: {out.shape}")